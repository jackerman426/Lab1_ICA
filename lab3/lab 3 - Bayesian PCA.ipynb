{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 3: Bayesian PCA\n",
      "\n",
      "### Machine Learning: Principles and Methods, November 2013\n",
      "\n",
      "* The lab exercises should be made in groups of three people, or at least two people.\n",
      "* The deadline is Friday, 13 December, 23:59.\n",
      "* Assignment should be sent to T.S.Cohen at uva dot nl (Taco Cohen). The subject line of your email should be \"[MLPM2013] lab#_lastname1\\_lastname2\\_lastname3\". \n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLPM2013] lab01\\_Kingma\\_Hu\", the attached file should be \"lab01\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* NOTE: test your code and make sure we can run your notebook / scripts!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.special as sp\n",
      "import numpy as np\n",
      "import pylab as P\n",
      "import sys\n",
      "import cPickle, gzip\n",
      "\n",
      "DEBUG = 0\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        self.N = N # number of data points        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(d, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(d, d)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, d)\n",
      "        self.sigma_w = np.random.randn(d, d)\n",
      "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(d, 1))\n",
      "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "        # initialize latent variable z\n",
      "        self.data = np.random.randn(d, N)\n",
      "        # expectation of tau\n",
      "        self.exp_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "\n",
      "    def __checkSizes(self):\n",
      "        if(self.means_z.shape != (self.d, self.N)):\n",
      "            print \"ERROR self.means_z.shape\"\n",
      "            sys.exit()\n",
      "        if(self.sigma_z.shape != (self.d, self.d)):\n",
      "            print \"ERROR self.sigma_z\"\n",
      "            sys.exit()\n",
      "        if(self.mean_mu.shape != (self.d, 1)):\n",
      "            print \"ERROR self.mean_mu\"\n",
      "            sys.exit()\n",
      "        if(self.sigma_mu.shape != (self.d, self.d)):\n",
      "            print \"ERROR elf.sigma_mu\"\n",
      "            sys.exit()\n",
      "        if(self.means_w.shape != (self.d, self.d)):\n",
      "            print \"ERROR self.means_w\"\n",
      "            sys.exit()\n",
      "        if(self.sigma_w.shape != (self.d, self.d)):\n",
      "            print \"ERROR self.sigma_w\"\n",
      "            sys.exit()\n",
      "        if(self.bs_alpha_tilde.shape != (self.d, 1)):\n",
      "            print \"ERROR self.bs_alpha_tilde\"\n",
      "            sys.exit()\n",
      "    \n",
      "    def __update_z(self, X):\n",
      "        \"\"\"\n",
      "        Q(Z) = prod_n N(z_n|m_z,sigma_z)\n",
      "            where m_z = <tau>*sigma_z|<tau>*<W^T>*(x_n-<mu>)\n",
      "                  sigma_x = (I+<tau>*<W^T*W>)^-1\n",
      "\n",
      "        \"\"\"    \n",
      "        #update sigma_z\n",
      "        self.sigma_z = np.linalg.inv(np.identity(self.d) + np.multiply(self.exp_tau, np.trace(self.sigma_w) + np.dot(self.means_w.T, self.means_w)))\n",
      "        # update mean_z\n",
      "        self.means_z = self.exp_tau*np.dot(np.dot(self.sigma_z, self.means_w.T) , (X - self.mean_mu))\n",
      "        if DEBUG:\n",
      "            print \"============= UPDATE_Z ==============\"\n",
      "            print \"self.means_z\\n\", self.means_z\n",
      "            print \"self.sigma_z\\n\", self.sigma_z\n",
      "    \n",
      "    def __update_mu(self, X):\n",
      "        \"\"\"\n",
      "        Q(mu) = N(mu|m_mu,sigma_mu)\n",
      "            where m_mu = <tau>*sigma_mu*sum_n(x_n-<W><z_n>)\n",
      "                  sigma_mu = (beta+N<tau>)^-1 * I\n",
      "        \"\"\"\n",
      "        #update sigma_mu\n",
      "        self.sigma_mu = (self.beta + self.N * self.exp_tau)**(-1) * np.identity(self.d)\n",
      "        #update mean_mu\n",
      "        self.mean_mu = self.exp_tau * np.dot(self.sigma_mu,np.sum(X - np.dot(self.means_w, self.means_z), axis=1)).reshape((self.d,1))\n",
      "\n",
      "        if DEBUG:\n",
      "            print \"============= UPDATE_MU ==============\"\n",
      "            print \"self.sigma_mu\\n\", self.sigma_mu\n",
      "            print \"self.sigma_mu\\n\", self.mean_mu\n",
      "    \n",
      "    def __update_w(self, X):\n",
      "        # sigma of w\n",
      "        a_diag = np.diagflat(self.a_alpha_tilde/self.bs_alpha_tilde)\n",
      "        sum_ = np.zeros((self.d, self.d))\n",
      "        for n in xrange(self.N):\n",
      "            sum_ += self.sigma_z + np.dot(self.means_z[:,n].reshape((self.d,1)), self.means_z[:,n].T.reshape((1,self.d)))\n",
      "        self.sigma_w = np.linalg.inv(a_diag + self.exp_tau * sum_)\n",
      "        # means of w\n",
      "        self_means_w = np.dot(self.exp_tau*self.sigma_w, np.dot(self.means_z, (X - self.mean_mu).T)).T\n",
      "        # debug prints\n",
      "        if DEBUG:\n",
      "            print \"============= UPDATE_W ==============\"\n",
      "            print \"self.sigma_w\\n\", self.sigma_w\n",
      "            print \"self.means_w\\n\", self.means_w\n",
      "\n",
      "    \n",
      "    def __update_alpha(self):\n",
      "        # a_alpha_tilted\n",
      "        self.a_alpha_tilde = self.a_alpha + self.d / 2\n",
      "        # bs_alpha_tilde\n",
      "        for i in xrange(self.d):\n",
      "            self.bs_alpha_tilde[i] = self.b_alpha + np.dot(self.means_w[i], self.means_w.T[i])/2\n",
      "        # debug prints\n",
      "        if DEBUG:\n",
      "            print \"============= UPDATE_alpha ==============\"\n",
      "            print \"self.a_alpha_tilde\\n\", self.a_alpha_tilde\n",
      "            print \"self.bs_alpha_tilde\\n\", self.bs_alpha_tilde\n",
      "\n",
      "    def __update_tau(self, X):\n",
      "        # update a_tau_tilde\n",
      "        self.a_tau_tilde = self.a_tau + (self.N * self.d) / 2\n",
      "        sum_ = 0\n",
      "        # update b_tau_tilde\n",
      "        for n in xrange(self.N):\n",
      "            sum_ += np.dot(X.T[n] , X.T[n]) + np.dot(self.means_z.T[n], self.means_z.T[n])\n",
      "            + np.trace((self.means_w.T*self.means_w) * (self.sigma_z + np.dot(self.means_z, self.means_z.T)))\n",
      "            + 2 * np.dot(np.dot(self.mean_mu.T, self.means_w), self.means_z.T[n])\n",
      "            -2 * np.dot(X.T[n], self.mean_mu)\n",
      "            -2 * np.dot(np.dot(X.T[n], self.means_w), self.means_z[:,n])\n",
      "        self.b_tau_tilde = self.b_tau + sum_ / 2\n",
      "        # debug prints\n",
      "        if DEBUG:\n",
      "            print \"============= UPDATE_tau ==============\"\n",
      "            print \"self.bs_alpha_tilde\\n\", self.a_tau_tilde\n",
      "            print \"self.b_tau_tilde\\n\", self.b_tau_tilde\n",
      "\n",
      "    def L(self, X):\n",
      "        L = 0.0\n",
      "        return L\n",
      "\n",
      "    def CheckFittedModel(self, X):\n",
      "        print \"Checking if the model is well fitted...\"\n",
      "        print \"Matrices must be identical:\"\n",
      "        print X\n",
      "        print np.dot(self.means_w,self.means_z) + self.mean_mu\n",
      "    \n",
      "    def fit(self, X, iterations):\n",
      "        print \"fitting the model...\"\n",
      "        for x in xrange(iterations):\n",
      "            if (x%(iterations/10.0)==0.0):\n",
      "                print \".\",\n",
      "            vPca.__checkSizes()\n",
      "            self.__update_z(X)\n",
      "            self.__update_mu(X)\n",
      "            self.__update_w(X)\n",
      "            self.__update_alpha()\n",
      "            self.__update_tau(X)\n",
      "        print \"\\n\",iterations, \"iterations done\"\n",
      "    def _blob(self,x,y,area,colour):\n",
      "        \"\"\"\n",
      "        Draws a square-shaped blob with the given area (< 1) at\n",
      "        the given coordinates.\n",
      "        Source: http://wiki.scipy.org/Cookbook/Matplotlib/HintonDiagrams\n",
      "        \"\"\"\n",
      "        hs = np.sqrt(area) / 2\n",
      "        xcorners = np.array([x - hs, x + hs, x + hs, x - hs])\n",
      "        ycorners = np.array([y - hs, y - hs, y + hs, y + hs])\n",
      "        P.fill(xcorners, ycorners, colour, edgecolor=colour)\n",
      "\n",
      "    def hinton(self, maxWeight=None):\n",
      "        \"\"\"\n",
      "        Draws a Hinton diagram for visualizing a weight matrix. \n",
      "        Temporarily disables matplotlib interactive mode if it is on, \n",
      "        otherwise this takes forever.\n",
      "        Source: http://wiki.scipy.org/Cookbook/Matplotlib/HintonDiagrams\n",
      "        \"\"\"\n",
      "        reenable = False\n",
      "        if P.isinteractive():\n",
      "            P.ioff()\n",
      "        P.clf()\n",
      "        height, width = self.sigma_w.shape\n",
      "        if not maxWeight:\n",
      "            maxWeight = 2**np.ceil(np.log(np.max(np.abs(self.sigma_w)))/np.log(2))\n",
      "\n",
      "        P.fill(np.array([0,width,width,0]),np.array([0,0,height,height]),'gray')\n",
      "        P.axis('off')\n",
      "        P.axis('equal')\n",
      "        for x in xrange(width):\n",
      "            for y in xrange(height):\n",
      "                _x = x+1\n",
      "                _y = y+1\n",
      "                w = self.sigma_w[y,x]\n",
      "                if w > 0:\n",
      "                    self._blob(_x - 0.5, height - _y + 0.5, min(1,w/maxWeight),'white')\n",
      "                elif w < 0:\n",
      "                    self._blob(_x - 0.5, height - _y + 0.5, min(1,-w/maxWeight),'black')\n",
      "        if reenable:\n",
      "            P.ion()\n",
      "        P.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.randn(4,2)\n",
      "vPca = BayesianPCA(4,2)\n",
      "vPca.fit(X, 10000)\n",
      "vPca.CheckFittedModel(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fitting the model...\n",
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10000 iterations done\n",
        "Checking if the model is well fitted...\n",
        "Matrices must be identical:\n",
        "[[ 1.1726042   1.07663318]\n",
        " [-0.30446722 -0.13621539]\n",
        " [ 0.64653298  0.69813552]\n",
        " [-0.30292211 -0.37505165]]\n",
        "[[ 1.13220812  1.09693628]\n",
        " [-0.26721246 -0.16958836]\n",
        " [ 0.63365363  0.6990038 ]\n",
        " [-0.33613743 -0.33580365]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. The Q-distribution (5 points)\n",
      "\n",
      "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that from the equation $\\ln p(D) = \\mathcal{L}(Q) + \\mathrm{KL}(Q||P)$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$Q$ approximates $p(\u03b8|D)$. In an ideal situation where the lower bound $L(Q)$ would be equal to the true log marginal likelihood $\\ln P(D)$, the $KL(Q||P)$ must be $0$. By taking the definition of $KL(Q||P)$ (eq.11) : $KL(Q||P) = - \\int Q(\\theta) \\ln \\frac{P(\\theta|D)}{Q(\\theta)} \\mathrm{d}\\theta$. We can see that $P(\u03b8|D)$ must be equal to $Q(\u0398)$ for $KL(Q||P)$ to be equal to zero. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. The mean-field approximation (15 points)\n",
      "\n",
      "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
      "\n",
      "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $ln Q_i$, from which $Q_i$ can easily be obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Answer\n",
      "\n",
      "Starting from equation (10) we have:\n",
      "\n",
      "$L(Q) = \\int Q(\\theta) \\ln \\cfrac{P(D,\\theta)}{Q(\\theta)}$, replacing $Q(\\theta)$, with $\\prod_i Q_i(\\theta_i)$, we are going to have:\n",
      "\\begin{align*}\n",
      "L(Q) &= \\int \\prod_i Q_i(\\theta_i) \\ln \\cfrac{P(D,\\theta)}{\\prod_i Q_i(\\theta_i)} \\mathrm{d}\\theta\\\\\n",
      "&= \\int \\prod_i Q_i(\\theta_i) ( \\ln P(D,\\theta) - \\ln \\prod_i Q_i(\\theta_i)) \\mathrm{d}\\theta\\\\\n",
      "&= \\int \\prod_i Q_i(\\theta_i) \\ln P(D,\\theta) \\mathrm{d}\\theta - \\int \\prod_i Q_i(\\theta_i) \\ln \\prod_i Q_i(\\theta_i) \\mathrm{d}\\theta\\\\\n",
      "&= \\int \\prod_i Q_i(\\theta_i) \\ln P(D,\\theta) \\mathrm{d}\\theta - \\int \\prod_i Q_i(\\theta_i) \\sum_i \\ln Q_i(\\theta_i) \\mathrm{d}\\theta\\\\\n",
      "&= \\int Q_j(\\theta_j) \\prod_{i \\neq j} Q_i(\\theta_i) \\ln P(D,\\theta) \\mathrm{d}\\theta - \\int Q_j(\\theta_j) \\prod_{i \\neq j} Q_i(\\theta_i) \\sum_i \\ln Q_j(\\theta_j) \\prod_{i \\neq j} Q_i(\\theta_i) \\mathrm{d}\\theta$ (1)\\\\\n",
      "&= \\int Q_j(\\theta_j) \\int \\left( \\prod_{i \\neq j} Q_i(\\theta_i) \\ln P(D,\\theta) \\right) \\mathrm{d}\\theta_i \\mathrm{d}\\theta_j - \\int Q_j(\\theta_j) \\ln Q_j(\\theta_j) \\mathrm{d}\\theta_j + const (2)\n",
      "\\end{align*}\n",
      ", where we define a distribution as in 10.7(Bishop book) $\\bar{P}(D,\\theta)$ so, $\\ln \\bar{P}(D,\\theta) = \\langle \\ln P(D,\\theta) \\rangle_{i \\neq j} + c$, where c constant.\n",
      "\n",
      "Now, we can write the first them of the equation (1) as:\n",
      "\n",
      "\\begin{equation}\n",
      "\\int \\prod_{i \\neq j} Q_i(\\theta_i) \\ln P(D,\\theta) \\mathrm{d}\\theta_i =  \\langle \\ln P(D,\\theta) \\rangle_{i \\neq j}\n",
      "\\end{equation}\n",
      "\n",
      "If we now transform (2) in this way knowing that $\\int c \\ln(a) - \\int c \\ln(b) = \\int c ln \\cfrac{a}{b}$, and using the above definitions we have:\n",
      "\n",
      "\\begin{equation}\n",
      "L(Q) = \\int Q_j(\\theta_j) \\cfrac{\\ln \\bar{P}(D,\\theta)}{Q_j(\\theta_j)} \\mathrm{d}\\theta_j$\n",
      "\\end{equation}\n",
      ", which is the KL divergence between $Q_j(\\theta_j)$ and $\\ln \\bar{P}(D,\\theta)\n",
      "\n",
      "Maximizing now $L(Q)$ means that we have to minimize the KL divergence, which occurs only for $Q_j(\\theta_j) = \\bar{P}(D,\\theta)$. From 10.9 (Bishop book) we also that the general expression for this solution is given by:\n",
      "\n",
      "\\begin{equation}\n",
      "\\ln Q_j^*(\\theta_j) = \\langle \\ln P(D,\\theta) \\rangle_{i \\neq j}\n",
      "\\end{equation}\n",
      ", removing the logarithm from the equation we have:\n",
      "\n",
      "\\begin{equation}\n",
      "Q_j^*(\\theta_j) = \\cfrac{\\exp(\\langle \\ln P(D,\\theta) \\rangle_{i \\neq j})}{\\int \\exp(\\langle \\ln P(D,\\theta) \\rangle_{i \\neq j}) \\mathrm{d}\\theta_j}\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. The log-probability (10 points)\n",
      "\n",
      "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?\n",
      "\n",
      "#### Answer\n",
      "\n",
      "The log-prob of the data and parameters are given from:\n",
      "\\begin{align*}\n",
      "\\ln P(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\alpha, \\tau, \\mu) =& \\ln\\left(\\prod_{n=1}^N P(x_{n}|z_{n},W,\\mu,\\tau)P(Z)P(W|a)P(a)P(\\mu)P(\\tau)\\right) \\\\\n",
      "=& \\ln \\left( \\prod_{n=1}^N \\left[ \\left( \\cfrac{\\tau}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace - \\cfrac{1}{2} \\tau ||x_n - w z_n - \\mu||^2\\right\\rbrace \\right] \\right. \\\\\n",
      "& \\prod_{n=1}^N \\left[ \\left( \\cfrac{1}{2\\pi}\\right)^{q/2} \\exp \\left\\lbrace -\\cfrac{1}{2}  ||z_n||^2 \\right\\rbrace \\right]\\\\\n",
      "& \\prod_{i=1}^q \\left[ \\left( \\cfrac{\\alpha_i}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2} \\alpha_i ||\\mathbf{w}_i||^2 \\right\\rbrace \\right] \\\\\n",
      "& \\prod_{i=1}^q \\left[ \\cfrac{b_{\\alpha}^{\\alpha_{\\alpha}} \\alpha_i^{\\alpha_{\\alpha - 1}}\\exp \\lbrace -b_{\\alpha}\\alpha_{\\alpha} \\rbrace } {\\Gamma(\\alpha_{\\alpha})} \\right]\\\\\n",
      "& \\left(\\cfrac{\\beta}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2}\\beta  ||\\mathbf{\\mu}||^2 \\right\\rbrace\\\\\n",
      "& \\left. \\cfrac{d_{\\tau}^{c_{\\tau}} \\tau^{c_{\\tau} - 1} \\exp \\lbrace -\\tau d_{\\tau} \\rbrace }{\\Gamma(c_{\\tau})} \\right)\\\\\n",
      "=& \\ln \\prod_{n=1}^N \\left[ \\left( \\cfrac{\\tau}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace - \\cfrac{1}{2} \\tau ||x_n - w z_n - \\mu||^2\\right\\rbrace \\right]\\\\\n",
      "& \\ln \\prod_{n=1}^N \\left[ \\left( \\cfrac{1}{2\\pi}\\right)^{q/2} \\exp \\left\\lbrace -\\cfrac{1}{2}  ||z_n||^2 \\right\\rbrace \\right]\\\\\n",
      "& \\ln \\prod_{i=1}^q \\left[ \\left( \\cfrac{\\alpha_i}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2} \\alpha_i ||\\mathbf{w}_i||^2 \\right\\rbrace \\right] \\\\\n",
      "& \\ln \\prod_{i=1}^q \\left[ \\cfrac{b_{\\alpha}^{\\alpha_{\\alpha}} \\alpha_i^{\\alpha_{\\alpha - 1}}\\exp \\lbrace -b_{\\alpha}\\alpha_{\\alpha} \\rbrace } {\\Gamma(\\alpha_{\\alpha})} \\right]\\\\\n",
      "& \\ln \\left( \\left(\\cfrac{\\beta}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2}\\beta  ||\\mathbf{\\mu}||^2 \\right\\rbrace \\right)\\\\\n",
      "& \\ln \\cfrac{d_{\\tau}^{c_{\\tau}} \\tau^{c_{\\tau} - 1} \\exp \\lbrace -\\tau d_{\\tau} \\rbrace }{\\Gamma(c_{\\tau})}\\\\\n",
      "=& \\cfrac{dN}{2} \\ln \\left( \\cfrac{\\tau}{2\\pi} \\right) + \\sum_{n=1}^N \\left( - \\cfrac{\\tau}{2} ||x_n - wz_n -\\mu ||^2 \\right)\\\\\n",
      "& + \\cfrac{qN}{2} \\ln \\left( \\cfrac{1}{2\\pi} \\right) + \\sum_{i=1}^N \\left( \\cfrac{1}{2} ||z_n||^2 \\right)\\\\\n",
      "& + \\sum_{i=1}^q \\cfrac{d}{2} \\ln \\cfrac{\\alpha_i}{2\\pi} + \\sum_{i=1}^q \\left( - \\cfrac{\\alpha_i}{2} ||w_i||^2 \\right)\\\\\n",
      "& + q \\ln \\cfrac{b_{\\alpha}^{\\alpha_0}}{\\Gamma(\\alpha_{\\alpha})} + \\sum_{i=1}^q \\left( \\ln \\alpha^{(\\alpha_{\\alpha} - 1)} - b_{\\alpha}\\alpha_{0} \\right)\\\\\n",
      "& + \\cfrac{d}{2} \\ln \\cfrac{\\beta}{2\\pi} - \\cfrac{\\beta}{2} ||\\mu||^2\\\\\n",
      "& + \\ln d_{\\tau}^{c_{\\tau}} + \\ln \\tau^{c_{\\tau} - 1} - \\tau d_{\\tau}\n",
      "\\end{align*}\n",
      "\n",
      "As mentioned in the paper the Bayesian framework does not determine a specific value for the number principal components. Rather, it estimates a posterior distribution over models. The $\\ln P(D,\\theta)$ determines the joint distribution of data and parameters. If we wanted to use this to assess the convergence we would have to use point estimates of the parameters as we cannot use distributions to evaluate $\\ln P(D,\\theta)$. However we don't know if a single value of $Q$ would become higher, when $Q(\\theta)$ tends to be equal to $\\ln P(\\theta|D)$. So we cannot use this method to assess convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
      "\n",
      "The following result may be useful:\n",
      "\n",
      "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = \\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We used $\\mathbf{X}$ for observed and $\\mathbf{Z}$ for latent\n",
      "\n",
      "\\begin{align*}\n",
      "L(Q)=&\\int Q(\\theta)\\ln\\frac{P(D,\\theta)}{Q(\\theta)}d\\theta\\\\\n",
      "    =&\\int Q(\\theta)\\ln P(D,\\theta) d\\theta - \\int Q(\\theta)\\ln Q(\\theta)d\\theta\\\\\n",
      "    =&\\mathbb{E}[\\ln P(D,\\theta)]- \\mathbb{E}[\\ln Q(\\theta)]\\\\\n",
      "    =&\\mathbb{E}[\\ln\\prod_{n=1}^N P(x_{n}|z_{n},\\mathbf{W},\\mathbf{\\mu},\\tau)] + \\mathbb{E}[\\ln P(Z)] + \\mathbb{E}[\\ln P(\\mathbf{W|\\alpha})] + \\mathbb{E}[\\ln P(\\alpha)] + \\mathbb{E}[\\ln P(\\mu)] + \\mathbb{E}[\\ln P(\\tau)]\\\\\n",
      "    &- \\mathbb{E}[\\ln Q(Z)] - \\mathbb{E}[\\ln Q(\\mathbf{W})] - \\mathbb{E}[\\ln Q(\\alpha)] - \\mathbb{E}[\\ln Q(\\mu)] - \\mathbb{E}[\\ln Q(\\tau)]\\\\\n",
      "    =& \\mathbb{E} \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N} (x_{n} | \\mathbf{W}z_{n} +\\mu,\\sigma^2 \\mathbf{I}_{d}) \\right) \\right] + \\mathbb{E} \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(z_{n}|0, I_q) \\right) \\right] + \\mathbb{E} \\left[ \\ln \\prod_{i=1}^q \\left[ \\left( \\cfrac{\\alpha_i}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2} \\alpha_i ||\\mathbf{w}_i||^2 \\right\\rbrace \\right] \\right] + \\mathbb{E} \\left[\\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right)  \\right]\\\\\n",
      "    &+\\mathbb{E} \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right] + \\mathbb{E} \\left[ \\ln \\left( \\mathcal{N}(\\mu|0, \\beta^{-1} I) \\right) \\right] + \\mathbb{E}\\left[ \\ln \\left( \\Gamma(\\tau|c_\\tau, d_\\tau ) \\right) \\right] - \\mathbb{E} \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(z_n| m_z^{(n)}, \\Sigma_z) \\right) \\right] - \\mathbb{E} \\left[ \\ln \\left( \\prod_{k=1}^{d}  \\mathcal{N}(\\widetilde{w}_k | m_w^{(k)}, \\Sigma_w) \\right) \\right]\\\\\n",
      "    &-\\mathbb{E} \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma(\\alpha_i| \\widetilde{a}_\\alpha, \\widetilde{b}_{\\alpha i} ) \\right)\\right] - \\mathbb{E} \\left[ \\ln \\left( \\mathcal{N}(\\mu | m_\\mu, \\Sigma_\\mu) \\right) \\right] - \\mathbb{E} \\left[ \\ln \\left( \\Gamma(\\tau | \\widetilde{a}_\\tau, \\widetilde{b}_\\tau ) \\right) \\right]\n",
      "\\end{align*}\n",
      "\n",
      "We take each term separately:\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{align*}\n",
      "\\mathbb{E} \\left[ \\ln  P(Z) \\right] = & \\mathbb{E} \\left[ \\ln \\left( \\prod_{n=1}^{N} \\mathcal{N}(z_n|0, I_q) \\right) \\right]   \\\\\n",
      "                                    =& \\sum_{n=1}^N \\mathbb{E} \\left( \\ln \\mathcal{N}(z_n|0, I_q) \\right) \\\\\n",
      "                                    =& \\sum_{n=1}^N \\mathbb{E} \\left(-\\frac{1}{2}(z_{n}-0)^T I_{q}^{-1} (z_{n}-0)-\\frac{q}{2} \\ln(2\\pi)-\\frac{1}{2} \\ln |I_q|  \\right)\n",
      "\\end{align*}\n",
      "\n",
      "Where $\\frac{q}{2} \\ln(2\\pi)$ and $\\frac{1}{2} \\ln |I_q|$ are constant and from matrix cookbook: $ \\mathbb{E}_P[x^{T}Ax] = \\text{Tr}(A \\Sigma) + m^{T} A m $ so we have:\n",
      "\n",
      "\\begin{align*}\n",
      "\\mathbb{E} \\left[ \\ln  P(Z) \\right] =& -\\frac{1}{2} \\mathbb{E} \\left[(z_{n}-0)^{T} I_{q}^{-1} (z_{n}-0)\\right]+const\\\\\n",
      "                                    =& -\\frac{1}{2} \\sum_{n=1}^N \\int Q(z_{n}) z_{n}^{T}I_{q}^{-1}z_{n} dz_{n} +const \\\\\n",
      "                                    =& -\\frac{1}{2} \\sum_{n=1}^N \\left( Tr(\\Sigma_{z}I_{q}^{-1})+m_{z}^T I_{q}^{-1}m_{z}^{(n)}+const \\right)\\\\\n",
      "                                    =& -\\frac{N}{2} Tr(\\Sigma_{z}) - \\frac{1}{2} \\sum_{n=1}^N m_{z}^{T} I_{q}^{-1}m_{z} +const\n",
      "\\end{align*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{align*}\n",
      "\\mathbb{E} \\left[ \\ln P(W|\\alpha) \\right] =& \\mathbb{E} \\left[ \\ln \\prod_{i=1}^q \\left[ \\left( \\cfrac{\\alpha_i}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2} \\alpha_i ||\\mathbf{w}_i||^2 \\right\\rbrace \\right] \\right]\\\\\n",
      "                                          =& \\sum_{n=1}^N \\int Q(\\alpha_{i}) \\frac{d}{2} \\ln \\left( \\frac{\\alpha_{i}}{2\\pi} \\right) d\\alpha_{i} - \\frac{1}{2} \\int Q(\\alpha_{i}) \\alpha_{i} w_{i} w_{i}^{T} d\\alpha_{i}\\\\\n",
      "                                          =& \\frac{d}{2} \\int \\ln \\left(\\frac{\\alpha_{i}}{2\\pi}\\right) d\\alpha_{i} - \\frac{1}{2} \\int\\int Q(\\alpha_{i})Q(w_{i})\\alpha_{i}w_{i}w_{i}^{T} d\\alpha_{i} dw_{i}\\\\\n",
      "                                          =& \\frac{d}{2} \\int \\ln(\\alpha_{i})Q(\\alpha_{i}) d\\alpha_{i} - \\frac{d}{2} \\ln(2\\pi)d\\alpha_{i} - \\frac{1}{2} \\int\\int \\alpha_{i} Q(\\alpha_{i}) d\\alpha_{i} w_{i}^{T} w_{i} Q(w_{i}) dw_{i}\\\\\n",
      "                                          =& \\frac{d}{2}<\\ln a_{i}> - \\frac{1}{2} <a_{i}> <w_{i}^Tw_{i}>  \n",
      "\\end{align*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{align*}\n",
      "\\mathbb{E} \\left[ \\ln P(\\alpha) \\right] =& \\mathbb{E} \\left[ \\ln \\left( \\prod_{i=1}^{q} \\Gamma ( \\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right) \\right]\\\\\n",
      "                                        =& \\mathbb{E} \\left[\\sum_{i=1}^q \\ln \\Gamma(\\alpha_i| a_{\\alpha}, b_{\\alpha}) \\right]\\\\\n",
      "                                        =& \\mathbb{E} \\left[\\sum_{i=1}^q \\ln\\left(\\frac{1}{\\Gamma(\\alpha_{\\alpha})} b_{\\alpha}^{\\alpha_{\\alpha}} \\alpha_{i}^{\\alpha_{\\alpha}-1} e^{-b_{\\alpha}\\alpha_{i}}\\right)\\right]\\\\\n",
      "                                        =& \\mathbb{E} \\left[\\sum_{i=1}^q \\left(\\ln \\frac{1}{\\Gamma(\\alpha_{\\alpha})}+ \\alpha_{\\alpha}\\ln b_{\\alpha}+(\\alpha_{\\alpha}-1)\\ln \\alpha_{i} - b_{\\alpha}\\alpha_{i} \\right)\\right]\\\\\n",
      "                                        =& \\sum_{i=1}^q \\int \\ln \\frac{1}{\\Gamma(\\alpha_{\\alpha})}Q(\\alpha_{i}) d\\alpha_{i} + \\sum_{i=1}^q \\int Q(\\alpha_{i})\\alpha_{\\alpha}\\ln b_{\\alpha} d\\alpha_{i} + \\sum_{i=1}^q \\int Q(\\alpha_{i})(\\alpha_{\\alpha}-1)\\ln \\alpha_{i} d\\alpha_{i} + \\sum_{i=1}^q \\int Q(\\alpha_{i})(-b_{\\alpha}\\alpha_{i})d\\alpha_{i}\\\\\n",
      "                                        =& const+(\\alpha_{\\alpha}-1) \\sum_{i=1}^q \\int Q(\\alpha_{i})\\ln \\alpha_{i} d\\alpha_{i} - b_{\\alpha} \\sum_{i=1}^q \\int Q(\\alpha_{i})\\alpha_{i} d\\alpha_{i}\\\\\n",
      "                                        =& const+(\\alpha_{\\alpha}-1) \\sum_{i=1}^q(\\psi(\\alpha_{\\alpha}) - \\ln b_{\\alpha}) - b_{\\alpha}\\sum_{i=1}^q <\\alpha_{i}>\\\\\n",
      "                                        =& const+(\\alpha_{\\alpha}-1) q \\left(\\frac{\\Gamma ' (\\alpha_{\\alpha})}{\\Gamma(\\alpha_{\\alpha})} - \\ln b_{\\alpha} \\right) - b_{\\alpha} q \\frac{\\alpha_{\\alpha}}{b_{\\alpha}}\\\\\n",
      "                                        =& const+(\\alpha_{\\alpha}-1) q \\left(\\frac{\\Gamma ' (\\alpha_{\\alpha})}{\\Gamma(\\alpha_{\\alpha})} - \\ln b_{\\alpha} \\right) - q \\alpha_{\\alpha} \n",
      "\\end{align*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{align*}\n",
      "\\mathbb{E} \\left[ \\ln P(\\mu) \\right] =& \\mathbb{E} \\left[ \\ln \\left( \\mathcal{N} ( \\mu|0, b^{-1}I) \\right) \\right]\\\\\n",
      "                                     =& \\mathbb{E} \\left[\\ln\\left((2\\pi)^{-\\frac{d}{2}}|\\beta^{-1}I|^{-\\frac{1}{2}} e^{-\\frac{1}{2}\\mu\\mu^{T}\\beta}\\right)\\right]\\\\\n",
      "                                     =& \\mathbb{E} \\left[-\\frac{d}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln(|\\beta^{-1}I|)-\\frac{1}{2}\\mu\\mu^{T}\\beta\\right]\\\\\n",
      "                                     =& const - \\frac{1}{2} \\int \\ln |\\beta^{-1}I| - \\frac{1}{2} \\int Q(\\mu)\\mu\\mu^{T}\\beta d\\mu\\\\\n",
      "                                     =& const - \\frac{\\beta}{2} \\int Q(\\mu)\\mu^{T}\\mu d\\mu\\\\\n",
      "                                     =& const - \\frac{\\beta}{2} <\\mu\\mu^{T}>\\\\\n",
      "                                     =& const - \\frac{\\beta}{2} Tr|\\Sigma_{\\mu}|+m_{\\mu}m_{\\mu}^{T}\n",
      "\\end{align*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{align*}\n",
      "\\mathbb{E} \\left[ \\ln P(\\tau) \\right] =& \\mathbb{E} \\left[ \\ln \\left( \\Gamma \\left ( \\tau|c_{\\tau}, d_{\\tau}\\right ) \\right) \\right]\\\\\n",
      "                                      =& \\mathbb{E} \\left[ \\ln \\left(\\frac{1}{\\Gamma(c_{\\tau})}+d_{\\tau}^{c_{\\tau}}\\tau^{c_{\\tau}-1}e^{-\\tau c_{\\tau}}\\right)\\right]\\\\\n",
      "                                      =& \\mathbb{E} \\left[ \\ln \\frac{1}{\\Gamma(c_{\\tau})}+c_{\\tau}\\ln d_{\\tau} + (c_{\\tau}-1)\\ln \\tau - \\tau c_{\\tau} \\right]\\\\\n",
      "                                      =& \\int \\ln \\frac{1}{\\Gamma (c_{\\tau})} Q(\\tau) d\\tau + \\int c_{\\tau}\\ln d_{\\tau} Q(\\tau) d_{\\tau} + (c_{\\tau}-1) \\int \\ln \\tau Q(\\tau) d\\tau - c_{\\tau} \\int \\tau Q(\\tau) d\\tau\\\\\n",
      "                                      =& (c_{\\tau}-1) <\\ln \\tau> - c_{\\tau} <\\tau>\\\\\n",
      "                                      =& (c_{\\tau}-1) (\\psi(c_{\\tau})-\\ln d_{\\tau}) - c_{\\tau} \\frac{c_{\\tau}}{d_{\\tau}} + const \\\\\n",
      "                                      =& (c_{\\tau}-1)(\\frac{\\Gamma ' (c_{\\tau})}{\\Gamma(c_{\\tau})}-\\ln d_{\\tau}) - \\frac{c_{\\tau}^{2}}{d_{\\tau}}+const\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the first term of the lower bound equation we have:\n",
      "\n",
      "\n",
      "\\begin{align*}\n",
      "& \\mathbb{E}(\\ln Q(\\theta)) =\\\\\n",
      "& \\mathbb{E}(\\ln \\prod_i Q_i(\\theta_i)) =\\\\\n",
      "& \\mathbb{E}_Q \\left( \\ln \\prod_{n=1}^N \\mathcal{N}(X_n|m_x,\\Sigma_x)\\right) + \\mathbb{E}_Q \\left( \\ln \\mathcal{N}(\\mathbf{\\mu}|\\mathbf{m_{\\mu}, \\mathbf{\\Sigma}_x})\\right) + \\mathbb{E}_Q \\left( \\ln \\prod_{k=1}^d \\mathcal{N} (\\tilde{w}_k|m_w^{(k)},\\Sigma_w)\\right)\\\\\n",
      "&+ \\mathbb{E}_Q \\left( \\ln \\prod_{i=1}^q \\Gamma (a_i | \\tilde{a}_a, \\tilde{b}_{ai})\\right) + \\mathbb{E}_Q \\left( \\ln \\Gamma (\\tau | \\tilde{a}_{\\tau}, \\tilde{b}_{\\tau})\\right)\n",
      "\\end{align*}\n",
      "\n",
      "\n",
      "Now, we are going to expand each term of the above equation:\n",
      "\n",
      "\\begin{align*}\n",
      "&\\mathbb{E}_Q \\left( \\ln \\prod_{n=1}^N \\mathcal{N}(X_n|m_x,\\Sigma_x)\\right) = \\\\\n",
      "&\\mathbb{E}_Q \\left(\\sum_{n=1}^N \\ln \\mathcal{N}(X_n|m_x,\\Sigma_x) \\right)=\\\\\n",
      "&\\sum_{n=1}^N \\mathbb{E}_Q \\left( \\ln \\left( \\left( \\cfrac{1}{2\\pi}\\right)^{q/2} \\mathbf{\\Sigma_x}^{-1/2} \\exp (-\\frac{1}{2}(\\mathbf{x_n} - \\mathbf{m_x})^T \\mathbf{\\Sigma_x}^{-1}(\\mathbf{x_n} - \\mathbf{m_x}) )\\right) \\right)=\\\\\n",
      "&\\mathbb{E}_Q\\left( \\cfrac{Nq}{2}\\ln \\left( \\cfrac{1}{2\\pi} \\right) \\right) + \\mathbb{E}_Q\\left( \\cfrac{q}{2} \\ln \\mathbf{\\Sigma_x}  \\right) + \\mathbb{E}_Q\\left( \\sum_{n=1}^N (-\\frac{1}{2}(\\mathbf{x_n} - \\mathbf{m_x})^T \\mathbf{\\Sigma_x}^{-1}(\\mathbf{x_n} - \\mathbf{m_x})) \\right) = \\\\\n",
      "&\\mathbb{E}_Q\\left( \\sum_{n=1}^N (-\\frac{1}{2}(\\mathbf{x_n} - \\mathbf{m_x})^T \\mathbf{\\Sigma_x}^{-1}(\\mathbf{x_n} - \\mathbf{m_x})) \\right) + const = \\\\\n",
      "&-\\cfrac{1}{2} \\sum_{n=1}^N \\int Q(x_n) (\\mathbf{x_n} - \\mathbf{m_x})^T \\mathbf{\\Sigma_x}^{-1}(\\mathbf{x_n} - \\mathbf{m_x})\\mathrm{d}x_n + const =,\\text{ 3.57 Matrix Cookbook}\\\\\n",
      "&= \\cfrac{N}{2} + Trace(\\Sigma_x \\Sigma_x^{-1}) + const = \\\\\n",
      "&= \\cfrac{Nq}{2} +const\n",
      "\\end{align*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{equation*}\n",
      "\\mathbb{E}_Q \\left( \\ln \\mathcal{N}(\\mathbf{\\mu}|\\mathbf{m_{\\mu}, \\mathbf{\\Sigma}_x})\\right) =  const\\text{, similar to the previous derivation.} \\\\\n",
      "\\end{equation*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{equation*}\n",
      "\\mathbb{E}_Q \\left( \\ln \\prod_{k=1}^d \\mathcal{N} (\\tilde{w}_k|m_w^{(k)},\\Sigma_w)\\right) =  const\\text{, similar to the previous two derivations.} \\\\\n",
      "\\end{equation*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{align*}\n",
      "& \\mathbb{E}_Q \\left( \\ln \\prod_{i=1}^q \\Gamma (a_i | \\tilde{a}_a, \\tilde{b}_{ai})\\right)= \\\\\n",
      "& \\mathbb{E}_Q\\left( \\sum_{i=1}^q \\ln \\Gamma (a_i | \\tilde{a}_a, \\tilde{b}_{ai}) \\right) =\\\\\n",
      "& \\mathbb{E}_Q\\left( \\sum_{i=1}^q \\ln \\left( \\cfrac{1}{\\Gamma(\\tilde{a}_a)} b_{\\tilde{a}i}^{\\tilde{a}_a} a_i^{\\tilde{a}_a -1} e^{-\\tilde{b}_{ai}a_i} \\right) \\right)=\\\\\n",
      "& \\mathbb{E}_Q\\left( \\sum_{i=1}^q \\left( - \\ln\\Gamma(\\tilde{a}_a) +\\ln b_{\\tilde{a}i}^{\\tilde{a}_a} +\\ln a_i^{\\tilde{a}_a -1} - \\tilde{b}_{ai}a_i  \\right) \\right)=\\\\\n",
      "& -\\int \\mathrm{d}a_i \\sum_{i=1}^q  Q(a_i) \\ln\\Gamma(\\tilde{a}_a) + \\int \\mathrm{d}a_i \\sum_{i=1}^q  Q(a_i) \\ln b_{\\tilde{a}i}^{\\tilde{a}_a} + \\int \\mathrm{d}a_i \\sum_{i=1}^q  Q(a_i) \\ln a_i^{\\tilde{a}_a -1} - \\int \\mathrm{d}a_i \\sum_{i=1}^q  Q(a_i) \\tilde{b}_{ai}a_i =\\\\\n",
      "&\\text{, in which the first two terms are constants...}\\\\\n",
      "&\\int \\mathrm{d}a_i \\sum_{i=1}^q  Q(a_i) \\ln a_i^{\\tilde{a}_a -1} - \\int \\mathrm{d}a_i \\sum_{i=1}^q  Q(a_i) \\tilde{b}_{ai}a_i +const=\\\\\n",
      "& (\\tilde{a}_a -1)\\sum_{i=1}^q \\int \\mathrm{d}a_i   Q(a_i) \\ln a_i - \\sum_{i=1}^q \\tilde{b}_{ai} \\int \\mathrm{d}a_i  Q(a_i) a_i +const=\\\\\n",
      "&(\\tilde{a}_a -1)\\sum_{i=1}^q \\mathbb{E}_Q[\\ln a_i]  - \\sum_{i=1}^q \\tilde{b}_{ai} \\mathbb{E}_Q[a_i]+const  =\\\\\n",
      "& (\\tilde{a}_a -1)\\sum_{i=1}^q \\left( \\cfrac{\\Gamma'(\\tilde{a}_a)}{\\Gamma(\\tilde{a}_a)} - \\ln(\\tilde{b}_{ai})\\right)  - \\sum_{i=1}^q \\tilde{b}_{ai} \\cfrac{\\tilde{a}_a}{\\tilde{b}_{ai}} +const\n",
      "\\end{align*}\n",
      "\n",
      "==============================================================================================================================================\n",
      "\\begin{align*}\n",
      "&\\mathbb{E}_Q \\left( \\ln \\Gamma (\\tau | \\tilde{a}_{\\tau}, \\tilde{b}_{\\tau})\\right) = \\\\\n",
      "&\\mathbb{E}_Q \\left( \\ln \\left( \\cfrac{1}{\\Gamma(\\tilde{a}_{\\tau})} \\tilde{b}_{\\tau}^{\\tilde{a}_{\\tau}} \\tau^{\\tilde{a}_{\\tau} -1 } e^{-\\tilde{b}_{\\tau} \\tau} \\right) \\right) = \\\\\n",
      "&\\mathbb{E}_Q \\left( \\ln \\cfrac{1}{\\Gamma(\\tilde{a}_{\\tau})} + \\ln \\tilde{b}_{\\tau}^{\\tilde{a}_{\\tau}}+\\ln \\tau^{\\tilde{a}_{\\tau} -1 } + \\ln e^{-\\tilde{b}_{\\tau} \\tau} \\right) = \\\\\n",
      "&\\mathbb{E}_Q \\left( \\ln \\cfrac{1}{\\Gamma(\\tilde{a}_{\\tau})} + \\ln \\tilde{b}_{\\tau}^{\\tilde{a}_{\\tau}}+\\ln \\tau^{\\tilde{a}_{\\tau} -1 } -\\tilde{b}_{\\tau} \\tau \\right) = \\\\\n",
      "& \\mathbb{E}_Q \\left( \\ln \\cfrac{1}{\\Gamma(\\tilde{a}_{\\tau})} \\right) + \\mathbb{E}_Q \\left( \\ln \\tilde{b}_{\\tau}^{\\tilde{a}_{\\tau}} \\right) + \\mathbb{E}_Q \\left( \\ln \\tau^{\\tilde{a}_{\\tau} -1 } \\right) + \\mathbb{E}_Q \\left( -\\tilde{b}_{\\tau} \\tau \\right) =\\\\\n",
      "&\\int \\mathrm{d}\\tau Q(\\tau)\\ln \\cfrac{1}{\\Gamma(\\tilde{a}_{\\tau})}  + \\int \\mathrm{d}\\tau Q(\\tau)\\ln \\tilde{b}_{\\tau}^{\\tilde{a}_{\\tau}}  + \\int \\mathrm{d}\\tau Q(\\tau) \\ln \\tau^{\\tilde{a}_{\\tau} -1 } + \\int \\mathrm{d}\\tau  Q(\\tau) -\\tilde{b}_{\\tau} \\tau=\\\\\n",
      "&\\text{, in which the first two terms are constants as they are independent of tau.}\\\\\n",
      "& \\int \\mathrm{d}\\tau Q(\\tau) \\ln \\tau^{\\tilde{a}_{\\tau} -1 } + \\int \\mathrm{d}\\tau  Q(\\tau) (-\\tilde{b}_{\\tau} \\tau) + const=\\\\\n",
      "& (\\tilde{a}_{\\tau} -1) \\int \\mathrm{d}\\tau Q(\\tau) \\ln \\tau - \\tilde{b}_{\\tau}\\int \\mathrm{d}\\tau  Q(\\tau) \\tau+ const=\\\\\n",
      "&(\\tilde{a}_{\\tau} -1) \\mathbb{E}_Q(\\ln \\tau) - \\tilde{b}_{\\tau} \\mathbb{E}_Q(\\tau)+ const=\\\\\n",
      "&(\\tilde{a}_{\\tau} -1)\\left( \\cfrac{\\Gamma'(\\tilde{a}_{\\tau})}{\\Gamma(\\tilde{a}_{\\tau})} - \\ln(\\tilde{b}_{\\tau}) \\right) - \\tilde{b}_{\\tau} \\cfrac{\\tilde{a}_{\\tau}}{\\tilde{b}_{\\tau}}+ const =\\\\\n",
      "&(\\tilde{a}_{\\tau} -1)\\left( \\cfrac{\\Gamma'(\\tilde{a}_{\\tau})}{\\Gamma(\\tilde{a}_{\\tau})} - \\ln(\\tilde{b}_{\\tau}) \\right) - \\tilde{a}_{\\tau}+ const\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5. Optimize variational parameters (50 points)\n",
      "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Learning algorithm (10 points)\n",
      "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
      "\n",
      "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
      "\n",
      "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean = np.zeros(10)\n",
      "cov = np.diag([5,4,3,2,1,1,1,1,1,1])\n",
      "X = np.random.multivariate_normal(mean,cov,100).T\n",
      "vPca = BayesianPCA(10,100)\n",
      "vPca.fit(X, 10000)\n",
      "# vPca.CheckFittedModel(X)\n",
      "# http://wiki.scipy.org/Cookbook/Matplotlib/HintonDiagrams\n",
      "vPca.hinton()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fitting the model...\n",
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7. PCA Representation of MNIST (10 points)\n",
      "\n",
      "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}